{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0020 | Total Loss 0.637718\n",
      "Iter 0040 | Total Loss 0.762712\n",
      "Iter 0060 | Total Loss 0.805768\n",
      "Iter 0080 | Total Loss 0.870600\n",
      "Iter 0100 | Total Loss 0.368864\n",
      "Iter 0120 | Total Loss 0.659126\n",
      "Iter 0140 | Total Loss 0.560290\n",
      "Iter 0160 | Total Loss 0.298013\n",
      "Iter 0180 | Total Loss 0.407210\n",
      "Iter 0200 | Total Loss 0.313286\n",
      "Iter 0220 | Total Loss 0.279348\n",
      "Iter 0240 | Total Loss 0.322765\n",
      "Iter 0260 | Total Loss 0.486933\n",
      "Iter 0280 | Total Loss 0.482868\n",
      "Iter 0300 | Total Loss 0.254792\n",
      "Iter 0320 | Total Loss 0.427532\n",
      "Iter 0340 | Total Loss 0.267018\n",
      "Iter 0360 | Total Loss 0.547727\n",
      "Iter 0380 | Total Loss 0.258040\n",
      "Iter 0400 | Total Loss 0.580842\n",
      "Iter 0420 | Total Loss 0.249644\n",
      "Iter 0440 | Total Loss 0.180842\n",
      "Iter 0460 | Total Loss 0.359119\n",
      "Iter 0480 | Total Loss 0.676958\n",
      "Iter 0500 | Total Loss 0.236674\n",
      "Iter 0520 | Total Loss 0.327736\n",
      "Iter 0540 | Total Loss 0.303796\n",
      "Iter 0560 | Total Loss 0.566056\n",
      "Iter 0580 | Total Loss 0.613506\n",
      "Iter 0600 | Total Loss 0.277182\n",
      "Iter 0620 | Total Loss 0.231397\n",
      "Iter 0640 | Total Loss 0.659571\n",
      "Iter 0660 | Total Loss 0.304340\n",
      "Iter 0680 | Total Loss 0.268489\n",
      "Iter 0700 | Total Loss 0.597801\n",
      "Iter 0720 | Total Loss 0.145156\n",
      "Iter 0740 | Total Loss 0.168342\n",
      "Iter 0760 | Total Loss 0.249987\n",
      "Iter 0780 | Total Loss 0.221369\n",
      "Iter 0800 | Total Loss 0.238015\n",
      "Iter 0820 | Total Loss 0.652611\n",
      "Iter 0840 | Total Loss 0.289934\n",
      "Iter 0860 | Total Loss 0.283052\n",
      "Iter 0880 | Total Loss 0.310160\n",
      "Iter 0900 | Total Loss 0.621994\n",
      "Iter 0920 | Total Loss 0.227355\n",
      "Iter 0940 | Total Loss 0.469440\n",
      "Iter 0960 | Total Loss 0.139293\n",
      "Iter 0980 | Total Loss 0.400401\n",
      "Iter 1000 | Total Loss 0.339677\n",
      "Iter 1020 | Total Loss 0.255855\n",
      "Iter 1040 | Total Loss 0.261661\n",
      "Iter 1060 | Total Loss 0.301792\n",
      "Iter 1080 | Total Loss 0.108445\n",
      "Iter 1100 | Total Loss 0.713729\n",
      "Iter 1120 | Total Loss 0.233235\n",
      "Iter 1140 | Total Loss 0.375947\n",
      "Iter 1160 | Total Loss 0.390962\n",
      "Iter 1180 | Total Loss 0.440601\n",
      "Iter 1200 | Total Loss 0.197902\n",
      "Iter 1220 | Total Loss 0.090158\n",
      "Iter 1240 | Total Loss 0.204392\n",
      "Iter 1260 | Total Loss 0.637898\n",
      "Iter 1280 | Total Loss 0.388203\n",
      "Iter 1300 | Total Loss 0.574593\n",
      "Iter 1320 | Total Loss 0.100799\n",
      "Iter 1340 | Total Loss 0.151709\n",
      "Iter 1360 | Total Loss 0.240042\n",
      "Iter 1380 | Total Loss 0.534496\n",
      "Iter 1400 | Total Loss 0.239795\n",
      "Iter 1420 | Total Loss 0.288508\n",
      "Iter 1440 | Total Loss 0.539559\n",
      "Iter 1460 | Total Loss 0.105366\n",
      "Iter 1480 | Total Loss 0.569073\n",
      "Iter 1500 | Total Loss 0.203095\n",
      "Iter 1520 | Total Loss 0.077147\n",
      "Iter 1540 | Total Loss 0.089776\n",
      "Iter 1560 | Total Loss 0.065510\n",
      "Iter 1580 | Total Loss 0.319405\n",
      "Iter 1600 | Total Loss 0.239946\n",
      "Iter 1620 | Total Loss 0.603451\n",
      "Iter 1640 | Total Loss 0.462776\n",
      "Iter 1660 | Total Loss 0.738019\n",
      "Iter 1680 | Total Loss 0.295022\n",
      "Iter 1700 | Total Loss 0.739181\n",
      "Iter 1720 | Total Loss 0.070064\n",
      "Iter 1740 | Total Loss 0.388840\n",
      "Iter 1760 | Total Loss 0.506299\n",
      "Iter 1780 | Total Loss 0.211131\n",
      "Iter 1800 | Total Loss 0.128310\n",
      "Iter 1820 | Total Loss 0.354248\n",
      "Iter 1840 | Total Loss 0.256313\n",
      "Iter 1860 | Total Loss 0.458343\n",
      "Iter 1880 | Total Loss 0.625389\n",
      "Iter 1900 | Total Loss 0.629657\n",
      "Iter 1920 | Total Loss 0.080405\n",
      "Iter 1940 | Total Loss 0.715681\n",
      "Iter 1960 | Total Loss 0.149946\n",
      "Iter 1980 | Total Loss 0.147445\n",
      "Iter 2000 | Total Loss 0.291390\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "\n",
    "args = Namespace(method='dopri5', data_size=1000, batch_time=10, batch_size=20, niters=2000, test_freq=20, viz=False, gpu=0, adjoint=False, device='cpu')\n",
    "\n",
    "if args.adjoint:\n",
    "    from torchdiffeq import odeint_adjoint as odeint\n",
    "else:\n",
    "    from torchdiffeq import odeint\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "true_y0 = torch.tensor([[2., 0.]]).to(args.device)\n",
    "t = torch.linspace(0., 25., args.data_size).to(args.device)\n",
    "true_A = torch.tensor([[-0.1, 2.0], [-2.0, -0.1]]).to(args.device)\n",
    "\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def forward(self, t, y):\n",
    "        return torch.mm(y**3, true_A)\n",
    "\n",
    "with torch.no_grad():\n",
    "    true_y = odeint(Lambda(), true_y0, t, method='dopri5')\n",
    "\n",
    "def get_batch(device):\n",
    "    s = torch.from_numpy(np.random.choice(np.arange(args.data_size - args.batch_time, dtype=np.int64), args.batch_size, replace=False))\n",
    "    batch_y0 = true_y[s]  # (M, D)\n",
    "    batch_t = t[:args.batch_time]  # (T)\n",
    "    batch_y = torch.stack([true_y[s + i] for i in range(args.batch_time)], dim=0)  # (T, M, D)\n",
    "    return batch_y0.to(device), batch_t.to(device), batch_y.to(device)\n",
    "\n",
    "\n",
    "class ODEFunc(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ODEFunc, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, 2),\n",
    "        )\n",
    "\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        return self.net(y**3)\n",
    "    \n",
    "\n",
    "func = ODEFunc().to(args.device)\n",
    "\n",
    "optimizer = optim.RMSprop(func.parameters(), lr=1e-3)\n",
    "\n",
    "for itr in range(1, args.niters + 1):\n",
    "    optimizer.zero_grad()\n",
    "    batch_y0, batch_t, batch_y = get_batch(args.device)\n",
    "    pred_y = odeint(func, batch_y0, batch_t).to(args.device)\n",
    "    loss = torch.mean(torch.abs(pred_y - batch_y))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if itr % args.test_freq == 0:\n",
    "        with torch.no_grad():\n",
    "            pred_y = odeint(func, true_y0, t)\n",
    "            loss = torch.mean(torch.abs(pred_y - true_y))\n",
    "            print('Iter {:04d} | Total Loss {:.6f}'.format(itr, loss.item()))\n",
    "\n",
    "    end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
